# Deep-Generalization-based-on-Types

Note: Read tbr_paper_B659.pdf for more details.

- Explainable AI concerns giving reasons for the choices made by an AI system. Many ways have been explored, ranging from using external tools for generating explanations to modifying the architecture of a classification or a regression model to account for interoperability.
- Sometimes, the efforts to improve interpretability have caused a negative impact on the accuracy of the predictions.
- This project discusses a more natural way of generating explanations. We follow the path of strong AI to build a system that thinks and reason like humans. 
- The system builds a knowledge graph based on observations and uses type abstraction to make inferences. 
- The observations contain partial information like any real-world data instance would. The AI system makes type inferences to fill those gaps and enrich the knowledge graph if and when available.
- The system also generates explanations for the inferences it makes. The explanations generated by the system align with how a human would reason about his choices.
